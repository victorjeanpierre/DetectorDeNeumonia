# -*- coding: utf-8 -*-
"""ModeloDeDeteccionDeNeumonia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmUxw1hBiFStskwUjx0oA-myue_51p5y
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np
import os
import zipfile
import pandas as pd
import SimpleITK as sitk
from sklearn.model_selection import train_test_split # Importa train_test_split
from sklearn.preprocessing import LabelEncoder # Importa LabelEncoder

# Descarga del conjunto de datos (si no lo has hecho ya)
!wget --no-check-certificate \
    "https://github.com/ieee8023/covid-chestxray-dataset/archive/refs/heads/master.zip" \
    -O "/tmp/chest_xray.zip"

local_zip = '/tmp/chest_xray.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

# Rutas y parámetros
image_size = (150, 150)
batch_size = 32
metadata_path = '/tmp/covid-chestxray-dataset-master/metadata.csv'
image_dir = '/tmp/covid-chestxray-dataset-master/images'

# Cargar las imágenes y etiquetas utilizando la función load_data
images, labels = load_data(metadata_path, image_dir, target_size=image_size)

# Convertir etiquetas de texto a numéricas usando LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Convertir etiquetas a one-hot encoding si es necesario
from tensorflow.keras.utils import to_categorical
labels = to_categorical(labels)

# Dividir los datos en conjuntos de entrenamiento y validación
train_images, val_images, train_labels, val_labels = train_test_split(
    images, labels, test_size=0.2, random_state=42
)

# Función para cargar datos
def load_data(metadata_path, image_dir, target_size=(150, 150)):
    metadata = pd.read_csv(metadata_path)
    images = []
    labels = []
    for index, row in metadata.iterrows():
        image_path = os.path.join(image_dir, row['filename'])

        # Verifica si el archivo existe
        if not os.path.exists(image_path):
            print(f"Archivo no encontrado: {image_path}")
            continue  # Salta a la siguiente imagen

        # Intenta abrir la imagen con SimpleITK si es .nii.gz
        if image_path.endswith('.nii.gz'):
            try:
                image_sitk = sitk.ReadImage(image_path)
                image_np = sitk.GetArrayFromImage(image_sitk)
                # Convierte la imagen a escala de grises si es necesario
                if image_np.ndim > 2:
                    image_np = image_np[0]  # Toma el primer canal
                # Redimensiona la imagen
                image = tf.keras.preprocessing.image.array_to_img(image_np)
                image = image.resize(target_size)
            except Exception as e:
                print(f"Error al leer la imagen {image_path}: {e}")
                continue  # Salta a la siguiente imagen
        else:
            # Si no es .nii.gz, intenta cargarla con load_img
            try:
                image = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)
            except Exception as e:
                print(f"Error al leer la imagen {image_path}: {e}")
                continue  # Salta a la siguiente imagen

        image_array = tf.keras.preprocessing.image.img_to_array(image)
        images.append(image_array)
        labels.append(row['finding'])
    return np.array(images), np.array(labels)

# Crear generadores de datos
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_generator = train_datagen.flow(
    train_images,
    train_labels,
    batch_size=batch_size
)

validation_generator = train_datagen.flow(
    val_images,
    val_labels,
    batch_size=batch_size
)

# Crear y compilar el modelo
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    # Cambio aquí para que coincida con el número de clases en tus etiquetas
    layers.Dense(train_labels.shape[1], activation='softmax')
])

# Compilar el modelo
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
epochs = 10
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator
)

# Evaluar el modelo
loss, accuracy = model.evaluate(validation_generator)
print(f"Pérdida: {loss:.4f}")
print(f"Precisión: {accuracy:.4f}")

# Graficar la precisión y pérdida durante el entrenamiento
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Precisión del entrenamiento')
plt.plot(epochs_range, val_acc, label='Precisión de la validación')
plt.legend(loc='lower right')
plt.title('Precisión del entrenamiento y validación')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Pérdida del entrenamiento')
plt.plot(epochs_range, val_loss, label='Pérdida de la validación')
plt.legend(loc='upper right')
plt.title('Pérdida del entrenamiento y validación')
plt.show()


# Entrenar el modelo
epochs = 20  # Puedes ajustar el número de épocas
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator
)


# Crear y compilar el modelo con más capas y neuronas
model = keras.Sequential([
    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(256, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(512, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5), # Agregar dropout para regularización
    layers.Dense(train_labels.shape[1], activation='softmax')
])

# Usar un optimizador más avanzado y un learning rate ajustado
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) # Ajustar learning rate

# Compilar el modelo
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo con más épocas
epochs = 100  # Aumentar el número de épocas
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator
)


# Evaluar el modelo
loss, accuracy = model.evaluate(validation_generator)
print(f"Pérdida: {loss:.4f}")
print(f"Precisión: {accuracy:.4f}")


# Evaluar el modelo
loss, accuracy = model.evaluate(validation_generator)
print(f"Pérdida: {loss:.4f}")
print(f"Precisión: {accuracy:.4f}")

# Graficar la precisión y pérdida durante el entrenamiento
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc)) # Use len(acc) to get the actual number of epochs

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Precisión del entrenamiento')
plt.plot(epochs_range, val_acc, label='Precisión de la validación')
plt.legend(loc='lower right')
plt.title('Precisión del entrenamiento y validación')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Pérdida del entrenamiento')
plt.plot(epochs_range, val_loss, label='Pérdida de la validación')
plt.legend(loc='upper right')
plt.title('Pérdida del entrenamiento y validación')
plt.show()

import requests
from PIL import Image
import numpy as np

# ... (Your existing code) ...

def predict_pneumonia(image_url):
    try:
        response = requests.get(image_url, stream=True)
        response.raise_for_status()  # Raise an exception for bad status codes

        image = Image.open(response.raw)
        image = image.resize((150, 150))  # Resize the image to match the model's input
        image_array = tf.keras.preprocessing.image.img_to_array(image)
        image_array = np.expand_dims(image_array, axis=0)  # Add a batch dimension
        image_array = image_array / 255.0  # Normalize the image

        prediction = model.predict(image_array)

        # Assuming your model outputs probabilities for different classes
        # Get the predicted class index
        predicted_class_index = np.argmax(prediction)

        # Get class labels from your label encoder
        class_labels = label_encoder.classes_

        predicted_class = class_labels[predicted_class_index]

        # You might need to adjust this based on your model's output
        if predicted_class == "Pneumonia":
            return "La imagen probablemente muestra neumonía."
        else:
            return "La imagen probablemente no muestra neumonía."

    except requests.exceptions.RequestException as e:
        return f"Error al descargar la imagen: {e}"
    except Exception as e:
        return f"Error al procesar la imagen: {e}"


# Example usage:
image_url = "https://mgyf.org/wp-content/uploads/2023/01/MGYF2022_061_f1.webp"  # Replace with the actual image URL
result = predict_pneumonia(image_url)
result

def train_model_with_feedback(model, train_generator, validation_generator, epochs=10):
    history = model.fit(
        train_generator,
        epochs=epochs,
        validation_data=validation_generator
    )
    return model, history

def get_feedback():
    """Gets feedback from the user on the model's performance."""
    while True:
        feedback = input("¿Fue correcta la predicción del modelo? (s/n): ").lower()
        if feedback in ['s', 'n']:
            return feedback
        else:
            print("Respuesta inválida. Por favor, responde con 's' o 'n'.")

def update_model(model, feedback, image, true_label):
    """Updates the model based on the user's feedback."""

    # Si la prediccion fue correcta no hacer nada.
    if feedback == 's':
        return

  
    print("La predicción fue incorrecta. Actualizando el modelo...")
   return model

import matplotlib.pyplot as plt
import numpy as np



def show_images_with_pneumonia_prediction(num_images=3):
    """Displays images from the training dataset along with their pneumonia predictions."""

    # Choose random indices from the training dataset
    indices = np.random.choice(len(train_images), num_images, replace=False)

    for i in indices:
        image = train_images[i]
        true_label = train_labels[i]

        # Preprocess the image for prediction
        image_array = np.expand_dims(image, axis=0)  # Add a batch dimension
        image_array = image_array / 255.0  # Normalize the image

        prediction = model.predict(image_array)
        predicted_class_index = np.argmax(prediction)
        predicted_class = label_encoder.classes_[predicted_class_index]

        # Decode the one-hot encoded true label
        true_class_index = np.argmax(true_label)
        true_class = label_encoder.classes_[true_class_index]

        plt.figure()
        plt.imshow(image.astype('uint8')) # Convert to uint8 for display
        plt.title(f"True: {true_class}, Predicted: {predicted_class}")
        plt.axis('off')
        plt.show()

# Example usage after training the model:
show_images_with_pneumonia_prediction()

# prompt: crea un codigo para que me muestre que imagenes a visto en su entrenamiento queno tiene neumonia 3 de 3

def show_non_pneumonia_images(num_images=3):
    """Displays images from the training dataset that are NOT pneumonia."""

    non_pneumonia_indices = np.where(np.argmax(train_labels, axis=1) == np.where(label_encoder.classes_ == "No Finding")[0][0])[0]

    if len(non_pneumonia_indices) == 0:
        print("No 'No Finding' images found in the training data.")
        return

    selected_indices = np.random.choice(non_pneumonia_indices, min(num_images, len(non_pneumonia_indices)), replace=False)

    for i in selected_indices:
        image = train_images[i]
        true_label = train_labels[i]

        # Preprocess the image for prediction (if needed)
        image_array = np.expand_dims(image, axis=0)
        image_array = image_array / 255.0

        prediction = model.predict(image_array)
        predicted_class_index = np.argmax(prediction)
        predicted_class = label_encoder.classes_[predicted_class_index]

        true_class_index = np.argmax(true_label)
        true_class = label_encoder.classes_[true_class_index]

        plt.figure()
        plt.imshow(image.astype('uint8'))
        plt.title(f"True: {true_class}, Predicted: {predicted_class}")
        plt.axis('off')
        plt.show()

# Example usage:
show_non_pneumonia_images()